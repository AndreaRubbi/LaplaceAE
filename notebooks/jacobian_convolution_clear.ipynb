{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d19c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../stochman/\")\n",
    "\n",
    "from stochman import nnj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f4c39",
   "metadata": {},
   "source": [
    "# Define convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d2cf76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "conv: \t\t\t weights torch.Size([5, 3, 7, 8]) \t- input torch.Size([11, 3, 20, 23]) \t- output torch.Size([11, 5, 16, 24])\n",
      "conv_dw: \t\t weights torch.Size([3, 11, 20, 23]) \t- input torch.Size([5, 3, 7, 8]) \t- output torch.Size([11, 5, 16, 24])\n",
      "conv_transposed: \t weights torch.Size([5, 3, 7, 8]) \t- input torch.Size([11, 5, 16, 24]) \t- output torch.Size([11, 3, 20, 23])\n",
      "conv_dw_transposed: \t weights torch.Size([3, 11, 20, 23]) \t- input torch.Size([5, 11, 16, 24]) \t- output torch.Size([5, 3, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "def compute_output_edge(input_edge, kernel_size=1,padding=0,stride=1,dilation=1):\n",
    "    output_edge = ( input_edge - dilation*(kernel_size-1) + 2*padding -1 )/stride +1 #output edge can be not-integer if stride!=1\n",
    "    return int(output_edge)\n",
    "def compute_output_padding(input_edge, output_edge, kernel_size=1,padding=0,stride=1,dilation=1):\n",
    "    return input_edge - ((output_edge-1)*stride - 2*padding + dilation*(kernel_size-1) + 1)\n",
    "def compute_reversed_padding(padding, kernel_size=1):\n",
    "    return kernel_size - 1 - padding\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "# Set parameters (free to change)\n",
    "batch_size = 11\n",
    "IN_c, OUT_c = 3, 5 #number of channels (for input and output)\n",
    "IN_h, IN_w = 20, 23 #number of pixels per input edges\n",
    "kernel_h, kernel_w = 7, 8\n",
    "set_padding_for_shape_preserving = False #not needed\n",
    "if set_padding_for_shape_preserving:\n",
    "    assert kernel_h%2==1 and kernel_w%2==1\n",
    "    padding_h, padding_w = int((kernel_h-1)/2), int((kernel_w-1)/2)\n",
    "else:\n",
    "    padding_h, padding_w = 1, 4\n",
    "stride = 1 #don't change me please, bad things will happen\n",
    "dilation = 1\n",
    "\n",
    "##############################################################################\n",
    "# Compute output sizes\n",
    "OUT_h, OUT_w = compute_output_edge(IN_h, kernel_size=kernel_h, padding=padding_h, stride=stride, dilation=dilation), compute_output_edge(IN_w, kernel_size=kernel_w, padding=padding_w, stride=stride, dilation=dilation)\n",
    "# Compute output padding for conv transpose\n",
    "out_padding_h, out_padding_w = compute_output_padding(IN_h, OUT_h, kernel_size=kernel_h, padding=padding_h, stride=stride, dilation=dilation), compute_output_padding(IN_w, OUT_w, kernel_size=kernel_w, padding=padding_w, stride=stride, dilation=dilation)\n",
    "dw_padding_h, dw_padding_w = compute_reversed_padding(padding_h, kernel_size=kernel_h), compute_reversed_padding(padding_w, kernel_size=kernel_w)\n",
    "dw_reversed_padding_h, dw_reversed_padding_w = compute_reversed_padding(padding_h, kernel_size=IN_h), compute_reversed_padding(padding_w, kernel_size=IN_w)\n",
    "print(out_padding_h, out_padding_w)\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "# Define convolutional layer\n",
    "conv = nnj.Conv2d(IN_c, \n",
    "                  OUT_c, \n",
    "                  kernel_size=(kernel_h, kernel_w), \n",
    "                  padding=(padding_h,padding_w), \n",
    "                  stride=stride, dilation=dilation, bias=None)\n",
    "assert list(conv.weight.shape) == [OUT_c, IN_c, kernel_h, kernel_w]\n",
    "# Define input images. Compute output images\n",
    "inputs = torch.randint(0, 10, (batch_size, IN_c, IN_h, IN_w)).type(torch.float)\n",
    "outputs = conv(inputs)\n",
    "assert list(outputs.shape) == [batch_size, OUT_c, OUT_h, OUT_w]\n",
    "print('conv: \\t\\t\\t weights',conv.weight.shape, '\\t- input',inputs.shape, '\\t- output',outputs.shape)\n",
    "\n",
    "# Define reversed convolutional layer (same operator as conv but with switched input/weights)\n",
    "conv_dw = nnj.ConvTranspose2d(IN_c, \n",
    "                              batch_size, \n",
    "                              kernel_size=(IN_h,IN_w), \n",
    "                              padding=(dw_padding_h,dw_padding_w), \n",
    "                              stride=stride, dilation=dilation, output_padding=0, bias=None)\n",
    "reversed_inputs = torch.flip(inputs, [-2,-1]).movedim(0,1)\n",
    "assert conv_dw.weight.shape == reversed_inputs.shape\n",
    "conv_dw.weight = torch.nn.Parameter(reversed_inputs)\n",
    "dw_outputs = torch.flip(conv_dw(conv.weight), [-2,-1]).movedim(0,1)\n",
    "assert torch.max(dw_outputs - outputs) < 1e-5 #chek that the two operators are actually the same\n",
    "print('conv_dw: \\t\\t weights',conv_dw.weight.shape, '\\t- input',conv.weight.shape, '\\t- output',dw_outputs.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define transposed of convolutional layer\n",
    "conv_transposed = nnj.ConvTranspose2d(OUT_c, \n",
    "                                      IN_c, \n",
    "                                      kernel_size=(kernel_h,kernel_w), \n",
    "                                      padding=(padding_h,padding_w), \n",
    "                                      stride=stride, dilation=dilation, bias=None, \n",
    "                                      output_padding=(out_padding_h,out_padding_w))\n",
    "conv_transposed.weight = conv.weight\n",
    "assert conv_transposed(outputs).shape == inputs.shape\n",
    "print('conv_transposed: \\t weights',conv_transposed.weight.shape, '\\t- input',outputs.shape, '\\t- output',conv_transposed(outputs).shape)\n",
    "\n",
    "# Define reversed transposed convolutional layer\n",
    "# maps:\n",
    "#   from - output space\n",
    "#   to - weight space\n",
    "conv_dw_transposed = nnj.Conv2d(batch_size, \n",
    "                                IN_c, \n",
    "                                kernel_size=(IN_h,IN_w), \n",
    "                                padding=(dw_padding_h,dw_padding_w), \n",
    "                                stride=stride, dilation=dilation, bias=None) # output_padding=0\n",
    "reversed_inputs = torch.flip(inputs, [-2,-1]).movedim(0,1)\n",
    "assert conv_dw_transposed.weight.shape == reversed_inputs.shape\n",
    "conv_dw_transposed.weight = torch.nn.Parameter(reversed_inputs)\n",
    "dw_weights = torch.flip(conv_dw_transposed(outputs.movedim(0,1)), [-2,-1])\n",
    "print('conv_dw_transposed: \\t weights',conv_dw_transposed.weight.shape, '\\t- input',outputs.movedim(0,1).shape, '\\t- output',dw_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d7e9d",
   "metadata": {},
   "source": [
    "# Jacobian of convolution [wrt. input]\n",
    "Computation of the full Jacobian (we don't want to do that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0df86123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 1920, 1380]) torch.Size([11, 1380, 1920])\n"
     ]
    }
   ],
   "source": [
    "IN_size, OUT_size = IN_c*IN_h*IN_w, OUT_c*OUT_h*OUT_w\n",
    "\n",
    "def conv_jacobian_wrt_input():\n",
    "    # define base elements on input, one for each batch size\n",
    "    output_identity = torch.eye(IN_size).unsqueeze(0).expand(batch_size,-1,-1)\n",
    "    assert list(output_identity.shape) == [batch_size, IN_size, IN_size]\n",
    "    # expand rows as cubes [(input channel)x(input height)x(input width)]\n",
    "    output_identity = output_identity.reshape(batch_size, IN_c, IN_h, IN_w, IN_size)\n",
    "    # define the shapes required by torch.conv2d\n",
    "    #  - from cube [(input channel)x(input height)x(input width)]\n",
    "    #  - to cube   [(output channel)x(output height)x(output width)]\n",
    "    input_for_shape = torch.zeros(batch_size,IN_c,IN_h,IN_w)\n",
    "    output_for_shape = torch.zeros(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "    # convolve each base element and compute the jacobian\n",
    "    jacobian = conv._jacobian_mult(input_for_shape, output_for_shape, output_identity)\n",
    "    # reshape as a (num of output)x(num of input) matrix, one for each batch size\n",
    "    jacobian = jacobian.reshape(batch_size, OUT_size, IN_size)\n",
    "    return jacobian\n",
    "\n",
    "def conv_jacobian_wrt_input_T():\n",
    "    # define base elements on output, one for each batch size\n",
    "    output_identity = torch.eye(OUT_size).unsqueeze(0).expand(batch_size,-1,-1)\n",
    "    assert list(output_identity.shape) == [batch_size, OUT_size, OUT_size]\n",
    "    # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "    output_identity = output_identity.reshape(batch_size, OUT_c, OUT_h, OUT_w, OUT_size)\n",
    "    # define the shapes required by torch.conv2d (transposed)\n",
    "    #  - from cube [(output channel)x(output height)x(output width)]\n",
    "    #  - to cube   [(input channel)x(input height)x(input width)]\n",
    "    input_for_shape = torch.zeros(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "    output_for_shape = torch.zeros(batch_size,IN_c,IN_h,IN_w)\n",
    "    # convolve each base element and compute the jacobian\n",
    "    jacobian = conv_transposed._jacobian_mult(input_for_shape, output_for_shape, output_identity)\n",
    "    # reshape as a (num of input)x(num of output) matrix, one for each batch size\n",
    "    jacobian = jacobian.reshape(batch_size, IN_size, OUT_size)\n",
    "    return jacobian\n",
    "\n",
    "# the two functions should return the same matrix, but transposed\n",
    "\n",
    "# check if the shape are one the transposed of the other\n",
    "print(conv_jacobian_wrt_input().shape, conv_jacobian_wrt_input_T().shape)\n",
    "assert conv_jacobian_wrt_input().shape == conv_jacobian_wrt_input_T().movedim(-1,-2).shape\n",
    "# check if the elements are the same\n",
    "assert torch.max( conv_jacobian_wrt_input() - conv_jacobian_wrt_input_T().movedim(-1,-2) ) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234d92b",
   "metadata": {},
   "source": [
    "# Jacobian of convolution [wrt. weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9f5c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 1920, 840]) torch.Size([11, 840, 1920])\n"
     ]
    }
   ],
   "source": [
    "def conv_jacobian_wrt_weight():\n",
    "    # define base elements on weights\n",
    "    output_identity = torch.eye(OUT_c*IN_c*kernel_h*kernel_w)\n",
    "    # expand rows as [(input channels)x(kernel height)x(kernel width)] cubes, one for each output channel\n",
    "    output_identity = output_identity.reshape(OUT_c, IN_c,kernel_h,kernel_w, OUT_c*IN_c*kernel_h*kernel_w)\n",
    "    # define the shapes required by torch.conv2d (transposed)\n",
    "    #  - from cube [(input channels)x(kernel height)x(kernel width)]\n",
    "    #  - to cube   [(batch size)x(output height)x(output width)]\n",
    "    # one for each output channel\n",
    "    input_for_shape = torch.zeros(OUT_c,IN_c,kernel_h,kernel_w)\n",
    "    output_for_shape = torch.zeros(OUT_c,batch_size,OUT_h,OUT_w)\n",
    "    # convolve each base element and compute the jacobian\n",
    "    jacobian = conv_dw._jacobian_mult(input_for_shape, output_for_shape, output_identity)\n",
    "    # transpose the result in (output height)x(output width)\n",
    "    jacobian = torch.flip(jacobian, [-3, -2])\n",
    "    # switch batch size and output channel\n",
    "    jacobian = jacobian.movedim(0,1)\n",
    "    # reshape as a (num of output)x(num of weights) matrix, one for each batch size\n",
    "    jacobian = jacobian.reshape(batch_size, OUT_size, OUT_c*IN_c*kernel_h*kernel_w)\n",
    "    return jacobian\n",
    "\n",
    "matrix_operator_from_input_to_output = conv_jacobian_wrt_input()\n",
    "output_computed_classically = torch.einsum('Bij,Bj->Bi',matrix_operator_from_input_to_output , inputs.reshape(batch_size,IN_size) ).reshape(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "matrix_operator_from_weight_to_output = conv_jacobian_wrt_weight()\n",
    "output_computed_reversed = torch.einsum('Bij,j->Bi', matrix_operator_from_weight_to_output, conv.weight.reshape(-1)).reshape(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "# check that conv and conv_dw actually represent the same operator\n",
    "assert torch.max(output_computed_classically - output_computed_reversed) < 1e-5\n",
    "\n",
    "def conv_jacobian_wrt_weight_T():\n",
    "    # define base elements on output, one for each batch size\n",
    "    output_identity = torch.eye(batch_size*OUT_size)\n",
    "    # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "    output_identity = output_identity.reshape(batch_size, OUT_c, OUT_h, OUT_w, batch_size*OUT_size)\n",
    "    # transpose the images in (output height)x(output width)\n",
    "    output_identity = torch.flip(output_identity, [-3, -2])\n",
    "    # switch batch size and output channel\n",
    "    output_identity = output_identity.movedim(0,1)\n",
    "    # define the shapes required by torch.conv2d\n",
    "    #  - from cube [(batch size)x(output height)x(output width)]\n",
    "    #  - to cube   [(input channels)x(kernel height)x(kernel width)]\n",
    "    input_for_shape = torch.zeros(OUT_c,batch_size,OUT_h,OUT_w)\n",
    "    output_for_shape = torch.zeros(OUT_c,IN_c,kernel_h,kernel_w)\n",
    "    # convolve each base element and compute the jacobian\n",
    "    jacobian = conv_dw_transposed._jacobian_mult(input_for_shape, output_for_shape, output_identity)\n",
    "    # reshape as a (num of weights)x(num of output) matrix\n",
    "    jacobian = jacobian.reshape(OUT_c*IN_c*kernel_h*kernel_w, batch_size, OUT_size)\n",
    "    # switch batch size and kernel size\n",
    "    jacobian = jacobian.movedim(0,1)\n",
    "    return jacobian\n",
    "\n",
    "# the two functions should return the same matrix, but transposed\n",
    "\n",
    "# check if the shape are one the transposed of the other\n",
    "print(conv_jacobian_wrt_weight().shape, conv_jacobian_wrt_weight_T().shape)\n",
    "assert conv_jacobian_wrt_weight().shape == conv_jacobian_wrt_weight_T().movedim(-1,-2).shape\n",
    "# check if the elements are the same\n",
    "assert torch.max( conv_jacobian_wrt_weight() - conv_jacobian_wrt_weight_T().movedim(-1,-2) ) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bcba9",
   "metadata": {},
   "source": [
    "# Assert jacobian wrt weights is correct\n",
    "Comparing to row-by-row computation (single layer NN, for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfb5e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asdfghjkl.gradient import batch_gradient\n",
    "\n",
    "def _flatten_after_batch(tensor: torch.Tensor):\n",
    "    if tensor.ndim == 1:\n",
    "        return tensor.unsqueeze(-1)\n",
    "    else:\n",
    "        return tensor.flatten(start_dim=1)\n",
    "        \n",
    "def _get_batch_grad(model):\n",
    "    batch_grads = list()\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'op_results'):\n",
    "            res = module.op_results['batch_grads']\n",
    "            if 'weight' in res:\n",
    "                batch_grads.append(_flatten_after_batch(res['weight']))\n",
    "            if 'bias' in res:\n",
    "                batch_grads.append(_flatten_after_batch(res['bias']))\n",
    "            if len(set(res.keys()) - {'weight', 'bias'}) > 0:\n",
    "                raise ValueError(f'Invalid parameter keys {res.keys()}')\n",
    "    return torch.cat(batch_grads, dim=1)\n",
    "    \n",
    "def jacobians(x, model, output_channel, output_h, output_w):\n",
    "    \"\"\"Compute Jacobians \\\\(\\\\nabla_\\\\theta f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n",
    "    using asdfghjkl's gradient per output dimension.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        input data `(batch, input_shape)` on compatible device with model.\n",
    "    Returns\n",
    "    -------\n",
    "    Js : torch.Tensor\n",
    "        Jacobians `(batch, parameters, outputs)`\n",
    "    f : torch.Tensor\n",
    "        output function `(batch, outputs)`\n",
    "    \"\"\"\n",
    "    Js = list()\n",
    "    for c in range(output_channel):\n",
    "        for i in range(output_h):\n",
    "            for j in range(output_w):\n",
    "                def loss_fn(outputs, targets):\n",
    "                    return outputs[:, c, i, j].sum()\n",
    "\n",
    "                f = batch_gradient(model, loss_fn, x, None).detach()\n",
    "                Jk = _get_batch_grad(model)\n",
    "\n",
    "                Js.append(Jk)\n",
    "    Js = torch.stack(Js, dim=1)\n",
    "    return Js, f\n",
    "\n",
    "# the two functions should return the same matrix\n",
    "jacobian_wrt_weight_slow, _ = jacobians(inputs, conv, OUT_c, OUT_h, OUT_w)\n",
    "jacobian_wrt_weight_fast = conv_jacobian_wrt_weight()\n",
    "\n",
    "# check if the shape are the same\n",
    "assert jacobian_wrt_weight_slow.shape == jacobian_wrt_weight_fast.shape\n",
    "# check if the elements are the same\n",
    "assert torch.max( jacobian_wrt_weight_slow - jacobian_wrt_weight_fast) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88b459",
   "metadata": {},
   "source": [
    "# Right and Left multiplications (J wrt to input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88f37140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 1380, 1380]) torch.Size([11, 1380, 1380]) torch.Size([11, 1380, 1380])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45529/4005062663.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# check if the elements are the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_Jt_tmp_J\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfast_Jt_tmp_J_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_Jt_tmp_J\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfast_Jt_tmp_J_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def conv_jacobian_wrt_input_T_right_multiply_to(tmp):\n",
    "    \"\"\"\n",
    "    Compute Jacobian^T * tmp\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tmp : torch.Tensor\n",
    "        input data `(batch_size, OUT_size, num_of_cols)`\n",
    "    Returns\n",
    "    -------\n",
    "    product : torch.Tensor\n",
    "        Jacobians `(batch_size, IN_size, num_of_cols)`\n",
    "    \"\"\"\n",
    "    num_of_cols = tmp.shape[-1]\n",
    "    assert list(tmp.shape) == [batch_size, OUT_size, num_of_cols]\n",
    "    # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "    tmp = tmp.reshape(batch_size, OUT_c, OUT_h, OUT_w, num_of_cols)\n",
    "    # define the shapes required by torch.conv2d (transposed)\n",
    "    #  - from cube [(output channel)x(output height)x(output width)]\n",
    "    #  - to cube   [(input channel)x(input height)x(input width)]\n",
    "    input_for_shape = torch.zeros(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "    output_for_shape = torch.zeros(batch_size,IN_c,IN_h,IN_w)\n",
    "    # convolve each column\n",
    "    Jt_tmp = conv_transposed._jacobian_mult(input_for_shape, output_for_shape, tmp)\n",
    "    # reshape as a (num of input)x(num of column) matrix, one for each batch size\n",
    "    Jt_tmp = Jt_tmp.reshape(batch_size, IN_size, num_of_cols)\n",
    "    return Jt_tmp\n",
    "\n",
    "def conv_jacobian_wrt_input_left_multiply_to(tmp):\n",
    "    \"\"\"\n",
    "    Compute tmp * Jacobian\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tmp : torch.Tensor\n",
    "        input data `(batch_size, num_of_rows, OUT_size)`\n",
    "    Returns\n",
    "    -------\n",
    "    product : torch.Tensor\n",
    "        Jacobians `(batch_size, num_of_rows, IN_size)`\n",
    "    \"\"\"\n",
    "    num_of_rows = tmp.shape[-2]\n",
    "    assert list(tmp.shape) == [batch_size, num_of_rows, OUT_size]\n",
    "    # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "    tmp_rows = tmp.movedim(-1,-2).reshape(batch_size, OUT_c, OUT_h, OUT_w, num_of_rows)\n",
    "    # see rows as columns of the transposed matrix\n",
    "    tmpt_cols = tmp_rows\n",
    "    # define the shapes required by torch.conv2d (transposed)\n",
    "    #  - from cube [(output channel)x(output height)x(output width)]\n",
    "    #  - to cube   [(input channel)x(input height)x(input width)]\n",
    "    input_for_shape = torch.zeros(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "    output_for_shape = torch.zeros(batch_size,IN_c,IN_h,IN_w)\n",
    "    # convolve each column\n",
    "    Jt_tmptt_cols = conv_transposed._jacobian_mult(input_for_shape, output_for_shape, tmpt_cols)\n",
    "    # reshape as a (num of input)x(num of output) matrix, one for each batch size\n",
    "    Jt_tmptt_cols = Jt_tmptt_cols.reshape(batch_size,IN_size,num_of_rows)\n",
    "    # transpose\n",
    "    tmp_J = Jt_tmptt_cols.movedim(1,2)\n",
    "    return tmp_J\n",
    "\n",
    "\n",
    "# define a random tmp matrix\n",
    "tmp = torch.randint(0, 10, (batch_size, OUT_size, OUT_size)).type(torch.float)\n",
    "\n",
    "# compute Jt*tmp*J defining the full jacobians (correct for sure but NOT memory efficient)\n",
    "slow_Jt_tmp_J = torch.einsum('Bji,Bjk,Bkq->Biq',conv_jacobian_wrt_input(),tmp,conv_jacobian_wrt_input())\n",
    "# compute (Jt*tmp)*J efficiently\n",
    "fast_Jt_tmp_J_1 = conv_jacobian_wrt_input_left_multiply_to(conv_jacobian_wrt_input_T_right_multiply_to(tmp))\n",
    "# compute Jt*(tmp*J) efficiently\n",
    "fast_Jt_tmp_J_2 = conv_jacobian_wrt_input_T_right_multiply_to(conv_jacobian_wrt_input_left_multiply_to(tmp))\n",
    "\n",
    "\n",
    "# check if the shape are the same\n",
    "print(slow_Jt_tmp_J.shape, fast_Jt_tmp_J_1.shape, fast_Jt_tmp_J_2.shape)\n",
    "# check if the elements are the same\n",
    "assert torch.abs(torch.max(slow_Jt_tmp_J - fast_Jt_tmp_J_1)) < 1e-5\n",
    "assert torch.abs(torch.max(slow_Jt_tmp_J - fast_Jt_tmp_J_2)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414abb9",
   "metadata": {},
   "source": [
    "# Right and Left multiplications (J wrt to weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235bf90",
   "metadata": {},
   "source": [
    "Right multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 56, 384]) torch.Size([11, 56, 384])\n"
     ]
    }
   ],
   "source": [
    "def conv_jacobian_wrt_weight_T_right_multiply_to(tmp, use_less_memory=True, conv_dw_transposed=conv_dw_transposed, inputs=inputs):\n",
    "    \n",
    "    if use_less_memory:\n",
    "        \"\"\"\n",
    "        Compute Jacobian^T * tmp\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tmp : torch.Tensor\n",
    "            input data `(batch_size, OUT_size, num_of_cols)`\n",
    "        Returns\n",
    "        -------\n",
    "        product : torch.Tensor\n",
    "            Jacobians `(batch_size, num_of_weights, num_of_cols)`\n",
    "        \"\"\"\n",
    "\n",
    "        conv_dw_transposed_single_batch = nnj.Conv2d(1, \n",
    "                                                    IN_c, \n",
    "                                                    kernel_size=(IN_h,IN_w), \n",
    "                                                    padding=(dw_padding_h,dw_padding_w), \n",
    "                                                    stride=stride, dilation=dilation, bias=None) # output_padding=0\n",
    "\n",
    "        num_of_cols = tmp.shape[-1]\n",
    "        assert list(tmp.shape) == [batch_size, OUT_size, num_of_cols]\n",
    "        # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "        tmp = tmp.reshape(batch_size, OUT_c, OUT_h, OUT_w, num_of_cols)\n",
    "        # transpose the images in (output height)x(output width)\n",
    "        tmp = torch.flip(tmp, [-3, -2])\n",
    "        # switch batch size and output channel\n",
    "        tmp = tmp.movedim(0,1)\n",
    "        # define the shapes required by torch.conv2d\n",
    "        #  - from cube [(batch size)x(output height)x(output width)]\n",
    "        #  - to cube   [(input channels)x(kernel height)x(kernel width)]\n",
    "        input_for_shape = torch.zeros(OUT_c,1,OUT_h,OUT_w)\n",
    "        output_for_shape = torch.zeros(OUT_c,IN_c,kernel_h,kernel_w)\n",
    "        # define moving sum for Jt_tmp\n",
    "        Jt_tmp = torch.zeros(batch_size, OUT_c*IN_c*kernel_h*kernel_w, num_of_cols)\n",
    "        for b in range(batch_size):\n",
    "            # set the weight to the convolution\n",
    "            input_single_batch = inputs[b,:,:,:].unsqueeze(0)\n",
    "            reversed_input_single_batch = torch.flip(input_single_batch, [-2,-1]).movedim(0,1)\n",
    "            assert conv_dw_transposed_single_batch.weight.shape == reversed_input_single_batch.shape\n",
    "            conv_dw_transposed_single_batch.weight = torch.nn.Parameter(reversed_input_single_batch)\n",
    "\n",
    "            tmp_single_batch = tmp[:,b,:,:,:].unsqueeze(1)\n",
    "            # convolve each column\n",
    "            Jt_tmp_single_batch = conv_dw_transposed_single_batch._jacobian_mult(input_for_shape, output_for_shape, tmp_single_batch)\n",
    "            # reshape as a (num of weights)x(num of column) matrix\n",
    "            Jt_tmp_single_batch = Jt_tmp_single_batch.reshape(OUT_c*IN_c*kernel_h*kernel_w, num_of_cols)\n",
    "            Jt_tmp[b, :, :] = Jt_tmp_single_batch\n",
    "        return Jt_tmp\n",
    "\n",
    "\n",
    "    else:    \n",
    "        \"\"\"\n",
    "        Compute Jacobian^T * tmp\n",
    "\n",
    "        WARNING: Works well with batch_size=1. With bigger batch_size you need to diagonal embed the batch sizes in a huge matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tmp : torch.Tensor\n",
    "            input data `(batch_size*OUT_size, num_of_cols)`\n",
    "        Returns\n",
    "        -------\n",
    "        product : torch.Tensor\n",
    "            Jacobians `(num_of_weights, num_of_cols)`\n",
    "        \"\"\"\n",
    "        num_of_cols = tmp.shape[-1]\n",
    "        assert list(tmp.shape) == [batch_size*OUT_size, num_of_cols]\n",
    "        # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "        tmp = tmp.reshape(batch_size, OUT_c, OUT_h, OUT_w, num_of_cols)\n",
    "        # transpose the images in (output height)x(output width)\n",
    "        tmp = torch.flip(tmp, [-3, -2])\n",
    "        # switch batch size and output channel\n",
    "        tmp = tmp.movedim(0,1)\n",
    "        # define the shapes required by torch.conv2d\n",
    "        #  - from cube [(batch size)x(output height)x(output width)]\n",
    "        #  - to cube   [(input channels)x(kernel height)x(kernel width)]\n",
    "        input_for_shape = torch.zeros(OUT_c,batch_size,OUT_h,OUT_w)\n",
    "        output_for_shape = torch.zeros(OUT_c,IN_c,kernel_h,kernel_w)\n",
    "        # convolve each column\n",
    "        Jt_tmp = conv_dw_transposed._jacobian_mult(input_for_shape, output_for_shape, tmp)\n",
    "        # reshape as a (num of weights)x(num of column) matrix\n",
    "        Jt_tmp = Jt_tmp.reshape(OUT_c*IN_c*kernel_h*kernel_w, num_of_cols)\n",
    "        return Jt_tmp\n",
    "\n",
    "# compute Jt*tmp defining the full jacobians (correct for sure but NOT memory efficient)\n",
    "slow_Jt_tmp = torch.einsum('Bji,Bjq->Biq',conv_jacobian_wrt_weight(),tmp)\n",
    "\n",
    "if batch_size==1:\n",
    "    # compute Jt*tmp efficiently\n",
    "    fast_Jt_tmp = conv_jacobian_wrt_weight_T_right_multiply_to(tmp[0], use_less_memory=False)\n",
    "\n",
    "    # check if the shape are the same\n",
    "    print(slow_Jt_tmp.shape, fast_Jt_tmp.shape)\n",
    "    # check if the elements are the same\n",
    "    assert torch.max(torch.abs(slow_Jt_tmp_J - fast_Jt_tmp_J_1)) < 1e-5\n",
    "\n",
    "else:\n",
    "    slow_Jt_tmp_sum = torch.zeros(slow_Jt_tmp.shape[1:])\n",
    "    for s in slow_Jt_tmp:\n",
    "        slow_Jt_tmp_sum =+ s\n",
    "\n",
    "    # compute Jt*tmp efficiently\n",
    "    fast_Jt_tmp = conv_jacobian_wrt_weight_T_right_multiply_to(tmp, use_less_memory=True)\n",
    "\n",
    "    # check if the shape are the same\n",
    "    print(slow_Jt_tmp.shape, fast_Jt_tmp.shape)\n",
    "    # check if the elements are the same\n",
    "    assert torch.max(torch.abs(slow_Jt_tmp - fast_Jt_tmp)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8098d",
   "metadata": {},
   "source": [
    "Left multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdaa0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 384, 56]) torch.Size([11, 384, 56])\n"
     ]
    }
   ],
   "source": [
    "def conv_jacobian_wrt_weight_left_multiply_to(tmp, use_less_memory=True, conv_dw_transposed=conv_dw_transposed, inputs=inputs):\n",
    "    \n",
    "    if use_less_memory:\n",
    "        \"\"\"\n",
    "        Compute tmp * Jacobian\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tmp : torch.Tensor\n",
    "            input data `(batch_size, num_of_rows, OUT_size)`\n",
    "        Returns\n",
    "        -------\n",
    "        product : torch.Tensor\n",
    "            Jacobians `(batch_size, num_of_rows, num_of_weights)`\n",
    "        \"\"\"\n",
    "\n",
    "        conv_dw_transposed_single_batch = nnj.Conv2d(1, \n",
    "                                                    IN_c, \n",
    "                                                    kernel_size=(IN_h,IN_w), \n",
    "                                                    padding=(dw_padding_h,dw_padding_w), \n",
    "                                                    stride=stride, dilation=dilation, bias=None) # output_padding=0\n",
    "\n",
    "        num_of_rows = tmp.shape[-2]\n",
    "        assert list(tmp.shape) == [batch_size, num_of_rows, OUT_size]\n",
    "        # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "        tmp_rows = tmp.movedim(-1,-2).reshape(batch_size, OUT_c, OUT_h, OUT_w, num_of_rows)\n",
    "        # see rows as columns of the transposed matrix\n",
    "        tmpt_cols = tmp_rows\n",
    "        # transpose the images in (output height)x(output width)\n",
    "        tmpt_cols = torch.flip(tmpt_cols, [-3, -2])\n",
    "        # switch batch size and output channel\n",
    "        tmpt_cols = tmpt_cols.movedim(0,1)\n",
    "        # define the shapes required by torch.conv2d\n",
    "        #  - from cube [(batch size)x(output height)x(output width)]\n",
    "        #  - to cube   [(input channels)x(kernel height)x(kernel width)]\n",
    "        input_for_shape = torch.zeros(OUT_c,1,OUT_h,OUT_w)\n",
    "        output_for_shape = torch.zeros(OUT_c,IN_c,kernel_h,kernel_w)\n",
    "        # define moving sum for Jt_tmp\n",
    "        tmp_J = torch.zeros(batch_size, OUT_c*IN_c*kernel_h*kernel_w, num_of_rows)\n",
    "        for b in range(batch_size):\n",
    "            # set the weight to the convolution\n",
    "            input_single_batch = inputs[b,:,:,:].unsqueeze(0)\n",
    "            reversed_input_single_batch = torch.flip(input_single_batch, [-2,-1]).movedim(0,1)\n",
    "            assert conv_dw_transposed_single_batch.weight.shape == reversed_input_single_batch.shape\n",
    "            conv_dw_transposed_single_batch.weight = torch.nn.Parameter(reversed_input_single_batch)\n",
    "\n",
    "            tmp_single_batch = tmpt_cols[:,b,:,:,:].unsqueeze(1)\n",
    "            # convolve each column\n",
    "            tmp_J_single_batch = conv_dw_transposed_single_batch._jacobian_mult(input_for_shape, output_for_shape, tmp_single_batch)\n",
    "            # reshape as a (num of weights)x(num of column) matrix\n",
    "            tmp_J_single_batch = tmp_J_single_batch.reshape(OUT_c*IN_c*kernel_h*kernel_w, num_of_rows)\n",
    "            tmp_J[b, :, :] = tmp_J_single_batch\n",
    "        # transpose\n",
    "        tmp_J = tmp_J.movedim(-1,-2)\n",
    "        return tmp_J\n",
    "\n",
    "\n",
    "    else:    \n",
    "        \"\"\"\n",
    "        Compute tmp * Jacobian\n",
    "\n",
    "        WARNING: Works well with batch_size=1. With bigger batch_size you need to diagonal embed the batch sizes in a huge matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tmp : torch.Tensor\n",
    "            input data `(num_of_rows, batch_size*OUT_size)`\n",
    "        Returns\n",
    "        -------\n",
    "        product : torch.Tensor\n",
    "            Jacobians `(num_of_rows, num_of_weights)`\n",
    "        \"\"\"\n",
    "        num_of_rows = tmp.shape[-2]\n",
    "        assert list(tmp.shape) == [num_of_rows, batch_size*OUT_size]\n",
    "        # expand rows as cubes [(output channel)x(output height)x(output width)]\n",
    "        tmp_rows = tmp.movedim(-1,-2).reshape(batch_size, OUT_c, OUT_h, OUT_w, num_of_rows)\n",
    "        # see rows as columns of the transposed matrix\n",
    "        tmpt_cols = tmp_rows\n",
    "        # transpose the images in (output height)x(output width)\n",
    "        tmpt_cols = torch.flip(tmpt_cols, [-3, -2])\n",
    "        # switch batch size and output channel\n",
    "        tmpt_cols = tmpt_cols.movedim(0,1)\n",
    "        # define the shapes required by torch.conv2d\n",
    "        #  - from cube [(batch size)x(output height)x(output width)]\n",
    "        #  - to cube   [(input channels)x(kernel height)x(kernel width)]\n",
    "        input_for_shape = torch.zeros(OUT_c,batch_size,OUT_h,OUT_w)\n",
    "        output_for_shape = torch.zeros(OUT_c,IN_c,kernel_h,kernel_w)\n",
    "        # convolve each column\n",
    "        Jt_tmptt_cols = conv_dw_transposed._jacobian_mult(input_for_shape, output_for_shape, tmpt_cols)\n",
    "        # reshape as a (num of input)x(num of output) matrix, one for each batch size\n",
    "        Jt_tmptt_cols = Jt_tmptt_cols.reshape(OUT_c*IN_c*kernel_h*kernel_w,num_of_rows)\n",
    "        # transpose\n",
    "        tmp_J = Jt_tmptt_cols.movedim(0,1)\n",
    "        return tmp_J\n",
    "\n",
    "# compute Jt*tmp defining the full jacobians (correct for sure but NOT memory efficient)\n",
    "slow_tmp_J = torch.einsum('Bij,Bjq->Biq',tmp,conv_jacobian_wrt_weight())\n",
    "\n",
    "if batch_size==1:\n",
    "    # compute Jt*tmp efficiently\n",
    "    fast_tmp_J = conv_jacobian_wrt_weight_left_multiply_to(tmp[0], use_less_memory=False)\n",
    "\n",
    "    # check if the shape are the same\n",
    "    print(slow_tmp_J[0].shape, fast_tmp_J.shape)\n",
    "    # check if the elements are the same\n",
    "    assert torch.max(torch.abs(slow_tmp_J[0] - fast_tmp_J)) < 1e-5\n",
    "\n",
    "else:\n",
    "    # compute Jt*tmp efficiently\n",
    "    fast_tmp_J = conv_jacobian_wrt_weight_left_multiply_to(tmp, use_less_memory=True)\n",
    "\n",
    "    # check if the shape are the same\n",
    "    print(slow_tmp_J.shape, fast_tmp_J.shape)\n",
    "    # check if the elements are the same\n",
    "    assert torch.max(torch.abs(slow_tmp_J - fast_tmp_J)) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba83003",
   "metadata": {},
   "source": [
    "# Diagonal Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21340155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 7, 8])\n",
      "torch.Size([5, 3, 7, 8]) torch.Size([5, 3, 7, 8])\n",
      "torch.Size([11, 1380])\n",
      "torch.Size([11, 1380])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABGCAYAAADVTc87AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWE0lEQVR4nO2dXYwk11XH/6eqv3t6Pndn1rvreO3Y+EMRIcGEhERRwEQYiGIeQMRSxIdAeQERUBAE3oiExANC8ICQrBCIBEqEQiSsKEqCEiIiEUV2Egkndow3xl7P7s7uzM7OTM9Mf1ZfHrrd53/uds/O7ppZt/v8XubW1K1bt27dul33X+ecKyEEOI7jOJNHcrsr4DiO49wcPoA7juNMKD6AO47jTCg+gDuO40woPoA7juNMKD6AO47jTCi3NICLyKMi8ryInBWRj79WlXIcx3Guj9ysHbiIpAD+B8D7AawCeArA4yGEZ1+76jmO4zjjyN3Cse8AcDaE8CIAiMhnATwGYOwAXkjKoZzW+hvpAS//ie4LckANhHbG+XhfRj9S8Wn59yuJCuHj+FQ9+6MX+Dj+QZSoPNonnS5XNsrX02SpMEz38rbypm1oVy+1xQlXtzfm/xFxuyd0XEa9Rno2H5cZqB6S2XxcRz4m6dp8XAbfq7QdVT7TivSKelBWjMrjW8V1iK7DEPetg/LyucbU/ZpzJSOzHTg/Fm6nA/p00qFjonvA/bhb1osMhahtE+q3UVukqV5MIdETNDr5kfUG7CPCx/f3jX7gc1G+HuVL6II7vagxKB+f69rzaBlZRjeuY/Nx/4yfs1xFd9byrWF6Kd01+eq90jDd6Gk79aI6FelkF5/d3gghHI8qfUsD+CkAr9D2KoCfPOiAclrDuxZ/ub8xN2N3Us8IZR20Qj4ejSgfDZy9or2UkNN9uV3txVnF5pP26AcfAHJ7ehwPnuluy+TLqjpKJG1t9BD9SElHz5WuXdEdSTQwN5vDdOfBNw3TjRN2NMryeo38ALbmbUdI25RuakflhxuwA0tWsvsKdT1u74TWN1+3D3tKZbbmtB7FLZuvuUQPINWvsm5HmXZNz8UPT+2cvQe5um7v3Fsbput32bbNtGuhtagXnN+1bTbux7FfX93JA2mIumpnTssXGghy+/ZcWVnbJvAPcXH8L2xxXTNmFZuPB+rKBTpmx+bL72v9Nt6iz0XjTtsxcjXdTnP2/izU9ofpUzPbw/QzF06afDxo92iQnaPjAaBFAz8fszxrB0H+gSjl9CZc3rHjSrer51qc1XO1OnYcSOgN5epVLSNds89c4areu/acbc9jb708TL/3xA+H6V9b+KbJ97W9B4bp5/bvGKZ3uwWT782VjWH6Ez/65MsYwa0M4KN+Kq/pcSLyEQAfAYBSMnPNAY7jOM7NcSsD+CqAO2n7NIALcaYQwhMAngCAucJykMFbaYjeOns1feXLKvpLJN14jq7JpKG/vGmwc+9Ybhjm27f5OrN6rrRh94WUdRNKR1VKG/qG0q3pL3Zup2ny8WwiO7k0TCeN6FW4rdv5qw0tuxrdrpqW16O38XiqzG/J/Jbdi4rjN0h+uweAxjHdzu/RlLUWvU1GlzI8/rjNV13TinSLuq+xZO9bZ5be4q/qeXuF6M16Ru9jcUvvY3PRTuW7K1yeltFasje1ckH3NY7b95L2ik4ZcptafvxmLV26J/T/5gnbz6RD15LTc6V74yWzxp1ahrRsPp5ZNak9u1Vbv/I6yQs8S6vbqUS6oP347adWzb5LDZ3tXN7X9JuXN0y+Ek3N9ulN81K9ZvLlU+28tZLOqrJIGtlva7vz2/TKXN3k+5E5fSu+1JgdpheK9s3/4r7ua3e1vL0t23+Sbkppswvrm1pGd5lmSNG7bi3VZ/p4Qet7b7lh8j1ceXGY/gRGcytWKE8BuE9E7haRAoAPAXjyFspzHMdxboCbfgMPIXRF5HcBfBlACuBTIYTvv2Y1cxzHcQ7kViQUhBC+COCLr1FdHMdxnBvglgbwG6YXEFp9sU3aViwV+qrM5k0S2akndRXrQkmr38vFOiBZAJDGHJdXIKuRWG/vzqoun9ZVB7zGMoa08rRJVigF27zdGb3GlCxcZN9q5aiWh8nGHfrhN7dnRbf6aS0vK5FVR6TNdSsYSS/SuQtkpVC6atuiUyGLH5IFK5ejNiM9uz0/WjcHgOai3i8hc82sEJltkTbL3x6ySANPW6qddujbAFvnAEAgkzjWvUPO1q+xjLHIHvc7Ku+4bQtp6bkL21xf23+65dHWJt256EZmY2xq59tmM2yS6SlZsiQ70fcKMnpgc8sQjQpd0phZ8was6dtyRfVcNikEgHP1BS2f/h+bB7ZYYyb70v3ILHGpqhr28bJaqGy3yibfS3X91pSypUnLPhSnKmpBs7o1r3XojGlzAJ0FW/dqRTX7ZdK256MH8mTu6jB9Jq/fCnZ61vSrE5s1jcBd6R3HcSYUH8Adx3EmlKOVUBKBlPpztVC1U52Q0nSBZI72nDVuT0k2YbkidtDpkQyT69FUOXaaSdn8zk5l2bEnaZMJYNlO51jy6RU0XyzJ5K/otC8UqYws8kar6/SruF4dplnS6VdKk221YEJh22ZrrMjIfYn1hUFnRvPt3WGnjpU1lrXomEpkOkczZ+OUktp87MVWJiefduQqwA5Z5U3y+qtbCY4lqRz3BbHTUOOJWaXKRlPlbE73JfXxU1njzdmO24xMyeh25yKnoZTMD1vLet7Chu3TaZPlKZIItyJ3UyqeHZQKkSNP2qFnhOrQno1kA7rh5Zxt95WS9tV6l0xoIwmlmleZp1ZQyXChYE3nnrmiji3nN1TKuP/kJZOP5ZXn1leG6ZOzOybfZkOlkuNVlVpOVuxD8t9X1PGoTc96bnf8Oy7LcQBwbGZvmF7Ja/mbmR0vmkG3ZxNtiy1YWackY2xyCX8DdxzHmVB8AHccx5lQjlhCSYByaeQutg7JSCbJ79ovuEk7G5mvuGZjJbRWdC7OMUni+CTs3VjYtl/zc9uqMQjJMBx3o18RmorSdVwjeZBSkpxVj7awNG+yZa/o9Evu01go+yciOYmMVyqXSMaJ4ghVyXkutsoYV7+cdVRDY5ksSsjZTaJpZJem36UN8ticsefN77NsMtojEACkOdoiiePK9P+hZbAVSmE7sn6hcEDFVfLmW7H9LN0hyawanaug28kOWaRUrWywd5pinOSp7t3oHtTo3FRee9GWl7S4H2t5+WZkaUOyDnvbhvh1jZqG43pw2QBQLGj9dlq2T7coshnLK7WcfUbmiiqVsOXK+f05k+/BRZVKOvPWm5PZ6Wg9fvrOF4bps3Ub7+muuc1hmi1PvrF6j823oJYhF1r6PObyUVtoNjT2bYOurqulzddmNN7J1qyVRn6irB6We0Gf6fOdBZPveGrloFH4G7jjOM6E4gO44zjOhOIDuOM4zoRytBp4ADDQkqVhNbJepTbiAEA62cj/A9bErDtntTn2zLMF2s3CFkU8i8wDC6uqn3WPq51eVrH58lvkpUk6bRyNsLOoWlg+O6bHnLNBHNN51QU7pPOHyBSPPSybi+z1aLIZL7sc7YsdvVgjjRcd4Ah3SZd0/mpscqbJNsUDz+9Gno7H2bRxfNzrtDV6X4gW30j29D42FvX7RycyicuKFAXxBF1Ub7yOfA3t0Ssw5K5GMaap+PaS9keJPCrTC6qDdpbpoG7kbdqg/l4d/82DTSW7FCu8sGtvKi+K0aG462HJfohYmNEPIieqVpflKINNMpeLzQjP71KfpgUTzpBGDQDfXjs9TFcKWjbH/AaAu2c1nv7z22pGuNux34lmaGEFjkDIZo0A0KVoh0lutHklAOyd0nT5HmuK+M6TGrL73XOqy//G7GVY9FzPtemDUtGOA2xuOA5/A3ccx5lQfAB3HMeZUI5WQkEYellmc1WzRyioFESnQe1FK41wQKeElkOLzQNZXgEFPoo9AgttntpGgZlWdNrHwbJiM0Ij89Bal6FiPeTSXZq20bkkH90GXnOTLiuWEzjQDs9Y4+k/L+hg2iWG1xWIJA+eprM0kDaiOvHMmXZ1I4/NuRfJHJQCWKWR8xnLOgldf7z8XY88W0vk2Rmft7BFizhwuxcib1heBzLy0kzJfKxbo/4TLTpAlm7mmKwcBb3KdF/xPPX9BStD9GityvJFvf5mFESLPTYLFMCqNWvrN3NB+6ptl2jqfkBgry651K43Zuj/0blIsrjc0nw7bft837uopoPdA3Qsllp4iTb2hgSsdFPJa+diGQewa2nSkrTXBPYqbGl655J1G760qDLw2aLKOudoYQbAema+1NXGvT9vpZYv7z1EW89jFP4G7jiOM6H4AO44jjOh+ADuOI4zoRytBt7N0Fvvm/8kse5L0QgLV9TtNojVH3tkwsfac9KzWmwgzVrIpDA2S5SMymhEv2dkEhgW1GYv3bCmVGGG7PmovrJvtfKETCd7m+qTm8xbd+LueTUnSndUOyxGpnNpmxdYJXPD6DI4cH+eItK15mx5vGhwFgW4y5FFpBygt7MrfMLR7pqxpk4LRORYAx9vUsgmirm9eEEQrRSbx8ULSWS04ERKkebStm20jKTZ7IS9j6GlDZrfJpf7YtQHZ2ixkCvUb0NkAkkae2dOBdh8PbqRpM3u06LGhSvRIsSkgRc3tU6VK5Gmzm1B9zfkraa+tq798z0rVs/9zqaua56jBROaUQS+pZJq0xyNsBfGf5PhSIJnZq25IS/owItHvHzFuqPPVvVcV2gh5ELOtkWbFpII23p/c5FJ7sx5Pa6xYtv9+TXVs398/twwHXfpc12tY5seoFRsxp+rPjtMfwyj8Tdwx3GcCcUHcMdxnAnlaCUUASQ3OGXRekyBF12Q8dOqdHv02pTJnvV6DAWdLvHamVkpMu3b1zl/L16ooTNaeokXo0CL1ums6tw7llB4EYdkWT0xext2ejhsIwAgKahTtbeL5YAuVekaU0kyI2RpILdvp2wsjcSemLwAA7iZonwcWZDX6eQ6ADAmhnwdrYWo7uSlmXRI8qjae5WSGWmLvAp7kdlkd4ZNKikdeUfyepTJpu2rLHmwAhB77aGhdezMsslrJOuQeWCe1s6MIyRyWxuzxEosH2qalndEL1rLtbQ5pl9s236WW9Rn6yvnHjD7Hjym0QP3u9pOF3ZnTb6NfTUb7pBckY+kjEKq22Uy+7u4b8vLyOzvalvP+8CKNcV7YUOfs0pRy2t17DWemlOvyqsFPVfzeLTWLtU9K9nOX6Jr2SUNcj2z48XLbRsx8VX+q2fHpksdllatl+awPiP/6ziO47zu8QHccRxnQjliCUUXdGArkf4+mt5xrKDIw5KD+ictskTIxeYQ9IV9nYLOHLNTMfYATYKdiraWddrHQa9ad9jAW+k+eV+yfFGzU6KspHUs/1A9zuSEnVLJprp7NRe0jNjio1OlaTTJEMUtex3tGu3bpsUIonhfLJOkrcjipcUWKiTdxOtzGM9RTWf5SKIgR9wcLYuYRAs6sBzAckhWsv0iV9cDTbCtaAELXuCgVzogiNauNkZxI+6rVAZ7qEZ1Z7mm+gpNvSP1sLWo96RTIyuULXvDu7SwhFDwrXiNzfaC5qtcpCBNkTkEr+XKdc+Wx0sD8xW7huUmLZJwcUefLZZJAKC5q/04X9bnth2t57lwh1p47TR1XzGSWlK2eCE5ZG3PPpssm/CamGfX7DM3f1yvS8grt7w2XtILi/aGL1HQrwVaEaWSWIup/Z52gIfJS/MHrZMm3zP1U7ge/gbuOI4zoVx3ABeRT4nIZRH5Hv1vUUT+XUReGPxdOKgMx3Ec57XnMG/g/wjg0eh/Hwfw1RDCfQC+Oth2HMdxjpDrauAhhP8UkTPRvx8D8L5B+tMAvg7gj697tkQghYH+07YmUr0ZFVO7ZCIWe9x151QXk874gOfprgqwbPaXbNvVenvzKsbyogAAUH5Rg8aHkupWxb14MQrdl9TV5Cqbt4uZCi1+3KNojMn6lskXKKJhnjwxu5V40QrydOSFGmIrTPqZ5uh8rXmbceEFPW9z3mqYbB7IDmM5K4kiT95+rNHH3qE18mirn9JuKJFHbZdMEUtXVZssXokiQtJhHNEwZ61LTT6OMhhr/p15PVfrWBTtr8FaPOnI0WLFKZkVNhd5cWZbpxyVx5p6fjvS71M2j+RFiMd7dvKiH0VrrWq+gZjvENFivXky7dtrWwG/WtX+ed/Sup63Z/tPY0Ev7PKuRvFLl6IokNR52dsyRJ2aPTjnauTZGa/YQqzVVR9/6OSa2be6Ox9nBwB0onVmeAGT0Ix0fvKGXmvr94CnGmdMvvO0aPIHa7qgQ7X0kj0ZVekzI2t38xr4SgjhIgAM/o4NOCkiHxGRp0Xk6XbWGJfNcRzHuUH+3z9ihhCeCCE8HEJ4uJCWr3+A4ziOcygkhPGmVMNMfQnlCyGEtwy2nwfwvhDCRRG5A8DXQwj3H6KcdQAvAzgGYOM62acFbwvF20Lxtujj7dDnrhDCNS6cN2sH/iSAXwfwF4O//3aYg16tgIg8HUJ4+CbP/YbC20LxtlC8Lfp4OxzMYcwIPwPgmwDuF5FVEfkt9Afu94vICwDeP9h2HMdxjpDDWKE8PmbXI69xXRzHcZwb4HZ5Yj5xm877esTbQvG2ULwt+ng7HMChPmI6juM4rz88ForjOM6EcqQDuIg8KiLPi8hZEZkq93sRuVNE/kNEnhOR74vIRwf/n9q4MiKSish3ReQLg+2pbAsRmReRz4nIDwb9411T3BZ/MHg+vicinxGR0rS2xWE4sgFcRFIAfwvg5wE8BOBxEXnoqM7/OqAL4GMhhAcBvBPA7wyuf5rjynwUwHO0Pa1t8TcAvhRCeADAW9Fvk6lrCxE5BeD3ADw88DlJAXwIU9gWh+Uo38DfAeBsCOHFEEIbwGfRj6kyFYQQLoYQvjNI19F/SE+h3wafHmT7NIBfuj01PFpE5DSAXwTwSfr31LWFiMwCeC+AvweAEEI7hLCFKWyLATkAZRHJAaigv5bYtLbFdTnKAfwUgFdoe3Xwv6lj4Nn6NgDfwg3ElXmD8dcA/gh2Vc1pbIt7AKwD+IeBnPRJEaliCtsihHAewF8COAfgIoDtEMJXMIVtcViOcgAfFSZs6kxgRGQGwL8C+P0Qws718r8REZEPALgcQvj27a7L64AcgLcD+LsQwtsA7GFKJYKBtv0YgLsBnARQFZEP395avb45ygF8FcCdtH0a45ZafoMiInn0B+9/DiF8fvDvS4N4Mhj8vTzu+DcQ7wbwQRF5CX0p7WdE5J8wnW2xCmA1hPCtwfbn0B/Qp7EtfhbA/4YQ1kMIHQCfB/BTmM62OBRHOYA/BeA+EblbRArof5x48gjPf1sREUFf53wuhPBXtOvVuDLADcSVmWRCCH8SQjgdQjiDfj/4Wgjhw5jOtlgD8IqIvBoM7hEAz2IK2wJ96eSdIlIZPC+PoP+taBrb4lAcqSOPiPwC+tpnCuBTIYQ/P7KT32ZE5D0AvgHgGaju+6fo6+D/AuBN6HfgXwkhbI4s5A2IiLwPwB+GED4gIkuYwrYQkR9D/2NuAcCLAH4T/ZeraWyLPwPwq+hbbX0XwG8DmMEUtsVhcE9Mx3GcCcU9MR3HcSYUH8Adx3EmFB/AHcdxJhQfwB3HcSYUH8Adx3EmFB/AHcdxJhQfwB3HcSYUH8Adx3EmlP8DMj/bwDQIRdMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1921e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "diag_tmp = torch.diagonal(tmp, dim1=1, dim2=2)\n",
    "\n",
    "# FAST METHOD\n",
    "print(conv.weight.shape)\n",
    "weigth_sq = conv.weight**2\n",
    "\n",
    "conv_sq = nnj.ConvTranspose2d(OUT_c,\n",
    "                              IN_c,\n",
    "                              kernel_size=(kernel_h,kernel_w),\n",
    "                              padding=(padding_h,padding_w),\n",
    "                              stride=stride,dilation=dilation, output_padding=0, bias=None)\n",
    "print(conv_sq.weight.shape , weigth_sq.shape)\n",
    "assert conv_sq.weight.shape == weigth_sq.shape\n",
    "conv_sq.weight = torch.nn.Parameter(weigth_sq)\n",
    "\n",
    "\n",
    "input_tmp = diag_tmp.reshape(batch_size,OUT_c,OUT_h,OUT_w)\n",
    "output_tmp = conv_sq._jacobian_mult(torch.zeros(batch_size, OUT_c,OUT_h,OUT_w), \n",
    "                                    torch.zeros(batch_size, IN_c, IN_h, IN_w), \n",
    "                                    input_tmp)\n",
    "diag_Jt_tmp_J = output_tmp.reshape(batch_size, IN_size)\n",
    "print(diag_Jt_tmp_J.shape)\n",
    "\n",
    "\n",
    "# SLOW METHOD\n",
    "tmp_simple = torch.diag_embed(diag_tmp)\n",
    "slow_Jt_tmp_J = torch.einsum('Bji,Bjk,Bkq->Biq', conv_jacobian_wrt_input(), tmp_simple, conv_jacobian_wrt_input())\n",
    "diag_slow_Jt_tmp_J = torch.diagonal(slow_Jt_tmp_J, dim1=1, dim2=2)\n",
    "print(diag_slow_Jt_tmp_J.shape)\n",
    "\n",
    "\n",
    "# compare\n",
    "plt.imshow(diag_slow_Jt_tmp_J[:,:100].detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(diag_Jt_tmp_J[:,:100].detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "print(torch.max(torch.abs(diag_slow_Jt_tmp_J - diag_Jt_tmp_J)))\n",
    "assert torch.max(torch.abs(diag_slow_Jt_tmp_J - diag_Jt_tmp_J)) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aec39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
